{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.1"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "427px"
      },
      "toc_section_display": true,
      "toc_window_display": true
    },
    "colab": {
      "name": "Homework 1.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1jmdmhAjkHq"
      },
      "source": [
        "\n",
        "\n",
        "# Developing Fair Classifiers: A case study using COMPAS data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1fhIA_YjkHw"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RKUzjX6jkHw"
      },
      "source": [
        "##### There are many different ways to measure fairness in AI. As such, there are many ways to adjust your AI models so that they better fit these fairness metrics. Here, you will examine this firsthand with the COMPAS dataset, which is a dataset that has been widely used to predict inmates' likelihood of recidivism (reoffense). First, you will run a simple classifier on this dataset and examine the results. Then, you will be asked how this classifier could be made more fair and, for extra credit, you can try implementing these changes yourself.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUSqhNCvjkHw"
      },
      "source": [
        "## Dataset exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "043ta_lx65N7"
      },
      "source": [
        "##### First, you will run the cells and answer the questions below in order to explore the contents of the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_BVlQfdjkHw"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57JEWrz1jkHx"
      },
      "source": [
        "#uncomment these lines if you need to install either of these two packages\n",
        "\n",
        "#!pip install plotly\n",
        "#!pip install aif360[LFR]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbNBFnJvjkHy"
      },
      "source": [
        "import urllib\n",
        "import urllib.request\n",
        "import os,sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from functools import reduce\n",
        "pd.set_option('display.max_columns',None)\n",
        "from pprint import pprint\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn.metrics as performance\n",
        "from sklearn.preprocessing import scale\n",
        "from random import seed, shuffle\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "try:\n",
        "    import plotly.express as px\n",
        "    import plotly.figure_factory as ff\n",
        "    import plotly.io as pio\n",
        "    import plotly.graph_objects as go\n",
        "    pio.renderers.default = \"notebook_connected\" #change to \"colab\" if using Google Colab, or \"notebook\" if offline\n",
        "except ImportError as e:\n",
        "#     !conda install --yes --prefix {sys.prefix} -c plotly plotly-orca==1.2.1 psutil requests\n",
        "    print(\"plotly is not installed !! please install the package to be able to render plotly visualizations\")\n",
        "\n",
        "\n",
        "try:\n",
        "    from aif360.metrics import BinaryLabelDatasetMetric\n",
        "    from aif360.metrics import ClassificationMetric\n",
        "    from aif360.metrics.utils import compute_boolean_conditioning_vector\n",
        "    from aif360.algorithms.postprocessing.calibrated_eq_odds_postprocessing import CalibratedEqOddsPostprocessing\n",
        "\n",
        "    from aif360.algorithms.preprocessing.optim_preproc_helpers.data_preproc_functions\\\n",
        "                    import load_preproc_data_compas\n",
        "    \n",
        "except ImportError as e:\n",
        "    print('please install aif360 package to proceed with the fairness section')\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vM-K4B-FjkHy"
      },
      "source": [
        "### Setting up visualization properties"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "G1DOUO0gjkHy"
      },
      "source": [
        "SMALL_SIZE = 8\n",
        "MEDIUM_SIZE = 10\n",
        "BIGGER_SIZE = 12\n",
        "fig=plt.figure(figsize=(10,6))\n",
        "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
        "plt.rc('axes', titlesize=MEDIUM_SIZE)     # fontsize of the axes title\n",
        "plt.rc('axes', labelsize=14)    # fontsize of the x and y labels\n",
        "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
        "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
        "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
        "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfjK9V_tjkH1"
      },
      "source": [
        "### UTILS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nX5ZHNHjkH2"
      },
      "source": [
        "def hist(column_name, x):\n",
        "    '''\n",
        "    helper function to plot historgram for a dataframe \n",
        "    '''\n",
        "    return px.histogram(column_name, x = x,\n",
        "                       title = x,\n",
        "                       opacity = 0.8,\n",
        "                       color_discrete_sequence = ['indianred'])\n",
        "def plot_roc(fpr,tpr,thresholds=None,label=None):\n",
        "    '''\n",
        "    plot roc curve for multiple instances.\n",
        "    '''\n",
        "    \n",
        "    colors = ['red','blue','aqua', 'darkorange', 'cornflowerblue', 'burlywood', 'lightsalmon', 'olive']\n",
        "\n",
        "    plt.figure(figsize=(12,8))\n",
        "    lw = 2\n",
        "    for i in range(len(fpr)):\n",
        "        plt.plot(fpr[i], tpr[i], color=colors[i],label=label[i],\n",
        "             lw=lw)\n",
        "        \n",
        "     \n",
        "    plt.plot([0, 1], [0, 1], color='black', lw=lw, linestyle='--')\n",
        "      \n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('ROC Curves of attributes ')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "#for each race/decile pair, calculate stats for recidivism\n",
        "def recid_count_fn(df_recid_column):\n",
        "    \"\"\" Computes the total number of examples in which recidivism occurred.\n",
        "        Parameters:\n",
        "            df_recid_column: DataFrame column where each row takes value 1 if\n",
        "            recidivism occured, and 0 if recidivism did not occur.\n",
        "        Returns:\n",
        "            The total number of examples in which recidivism occurred.\n",
        "    \"\"\"\n",
        "    recid_count = df_recid_column.sum()\n",
        "    return recid_count\n",
        "\n",
        "def non_recid_count_fn(df_recid_column):\n",
        "    \"\"\"Computes the total number of examples in which recidivism did not occur.\n",
        "    Parameters:\n",
        "        df_recid_column: dataframe column where each row takes value 1 if\n",
        "        recidivism occured, and 0 if recidivism did not occur.\n",
        "    Returns:\n",
        "        The total number of examples in which recidivism did not occur.\n",
        "    \"\"\"\n",
        "    non_recid_count = sum(df_recid_column == 0) \n",
        "    return non_recid_count\n",
        "\n",
        "def total_count_fn(df_recid_column):\n",
        "    \"\"\"Computes the total number of examples in the dataset.\n",
        "    Parameters:\n",
        "        df_recid_column: dataframe column where each row takes value 1 if\n",
        "        recidivism occured, and 0 if recidivism did not occur.\n",
        "    Returns:\n",
        "        The total number of examples in the dataset.\n",
        "    \"\"\"\n",
        "    total_count = df_recid_column.count()\n",
        "    return total_count\n",
        "\n",
        "\n",
        "#compute the classifier outcomes for different decision thresholds (using summary DataFrame above)\n",
        "#often in statistics, we can express the probabilities in the introduction in terms of true positive rate, \n",
        "#true negative rate, false positive rate, and false negative rate\n",
        "def TP(compas_stats_df):\n",
        "    \"\"\"\n",
        "        Args:\n",
        "            compas_stats_df: DataFrame found above\n",
        "        Returns:\n",
        "            An array of the number of true positives for each decile_score. We classify an example as 1 if \n",
        "            that example's score is equal to or above the decision threshold. Take recid_count. \n",
        "    \"\"\"\n",
        "    TPs = []\n",
        "    for threshold in compas_stats_df['decile_score']:\n",
        "        x = compas_stats_df[compas_stats_df['decile_score'] >= threshold]['recid_count'].sum()\n",
        "        true_positives = x\n",
        "        TPs.append(true_positives)\n",
        "    return np.array(TPs, dtype=np.int32)\n",
        "\n",
        "def FP(compas_stats_df):\n",
        "    \"\"\"\n",
        "        Args:\n",
        "            compas_stats_df: DataFrame found above\n",
        "        Returns:\n",
        "            An array of the number of false positives for each decile_score. We classify an example as 1 if \n",
        "            that example's score is equal to or above the decision threshold. Take non_recid_count. \n",
        "    \"\"\"\n",
        "    FPs = []\n",
        "    for threshold in compas_stats_df['decile_score']:\n",
        "        x = compas_stats_df[compas_stats_df['decile_score'] >= threshold]['non_recid_count'].sum()\n",
        "        false_positives = x\n",
        "        FPs.append(false_positives)\n",
        "    return np.array(FPs, dtype=np.int32)\n",
        "\n",
        "def TN(compas_stats_df):\n",
        "    \"\"\"\n",
        "        Args:\n",
        "            compas_stats_df: DataFrame found above\n",
        "        Returns:\n",
        "            An array of the number of true negatives for each decile_score. We classify an example as 1 if \n",
        "            that example's score less than the decision threshold. Take non_recid_count. \n",
        "    \"\"\"\n",
        "    TNs = []\n",
        "    for threshold in compas_stats_df['decile_score']:\n",
        "        x = compas_stats_df[compas_stats_df['decile_score'] < threshold]['non_recid_count'].sum()\n",
        "        true_negatives = x\n",
        "        TNs.append(true_negatives)\n",
        "    return np.array(TNs, dtype=np.int32)\n",
        "\n",
        "def FN(compas_stats_df):\n",
        "    \"\"\"\n",
        "        Args:\n",
        "            compas_stats_df: DataFrame found above\n",
        "        Returns:\n",
        "            An array of the number of true positives for each decile_score. We classify an example as 1 if \n",
        "            that example's score is equal to or above the decision threshold. Take recid_count. \n",
        "    \"\"\"\n",
        "    FNs = []\n",
        "    for threshold in compas_stats_df['decile_score']:\n",
        "        x = compas_stats_df[compas_stats_df['decile_score'] < threshold]['recid_count'].sum()\n",
        "        false_negatives = x\n",
        "        FNs.append(false_negatives)\n",
        "    return np.array(FNs, dtype=np.int32)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3wdcXNYjkH2"
      },
      "source": [
        "### Download data file and dependency files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0iONiuLnjkH2"
      },
      "source": [
        "SEED = 1234\n",
        "seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "def check_data_file(fname):\n",
        "    files = os.listdir(\".\") # get the current directory listing\n",
        "    print(\"Looking for file %s in the current directory...\" % (fname))\n",
        "\n",
        "    if fname not in files:\n",
        "        print(\" %s not found! Downloading from GitHub...\" % fname)\n",
        "        addr = \"https://raw.githubusercontent.com/propublica/compas-analysis/master/compas-scores-two-years.csv\"\n",
        "        response = urllib.request.urlopen(addr)\n",
        "        data = response.read()\n",
        "        fileOut = open(fname, \"wb\")\n",
        "        fileOut.write(data)\n",
        "        fileOut.close()\n",
        "        print(\"%s download and saved locally..\" % fname)\n",
        "    else:\n",
        "        print(\"File found in current directory..\")\n",
        "    \n",
        "COMPAS_INPUT_FILE = \"compas-scores-two-years.csv\"\n",
        "check_data_file(COMPAS_INPUT_FILE)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "J1AOWe3EjkH3"
      },
      "source": [
        "### Load and clean data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "DwTsFy-DjkH3"
      },
      "source": [
        "dataset = pd.read_csv(COMPAS_INPUT_FILE)\n",
        "\n",
        "dataset = dataset.dropna(subset=[\"days_b_screening_arrest\"]) # dropping missing vals\n",
        "\n",
        "dataset = dataset[ (dataset.days_b_screening_arrest <= 30) &\n",
        "(dataset.days_b_screening_arrest >= -30) &\n",
        "(dataset.is_recid != -1) & (dataset.c_charge_degree != 'O') & (dataset.score_text != 'N/A') ]\n",
        "\n",
        "dataset.reset_index(inplace=True, drop=True) # renumber the rows from 0 again"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "scrolled": false,
        "id": "GRkkfYKAjkH3"
      },
      "source": [
        "dataset.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kG49svx1jkH4"
      },
      "source": [
        "### Basic analysis of demographics\n",
        "A series of plots is given below which look at the distribution of the data, the decile scores (the scores in the COMPAS dataset that measure likelihood of reoffense), and the risk category, which is based on the decile score and is determined in the following manner:\n",
        "\n",
        " 1 – 4: scale score is low relative to other offenders in norm group.\n",
        "\n",
        " 5 – 7: scale score is medium relative to other offenders in norm group.\n",
        "\n",
        " 8 – 10: scale score is high relative to other offenders in norm group.\n",
        "\n",
        "\n",
        "After looking at the plots, you will be asked to describe the imbalances of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "X7Q-o2A8jkH4"
      },
      "source": [
        "def get_basic_stats(column_name,check_values=None,\n",
        "                    check_NaN=False,group_count=True,plot_hist = False):\n",
        "    '''\n",
        "    basic stats on a given column of the dataset\n",
        "    @param column_name: column name to compute stats on\n",
        "    '''\n",
        "    \n",
        "    result = {}\n",
        "    \n",
        "    if column_name not in dataset.columns:\n",
        "        raise ValueError(\"column_name must be set to a value from the the available values\")\n",
        "        \n",
        "    #values in the column\n",
        "    column_values = np.unique(dataset[column_name].values)\n",
        "    \n",
        "    if check_values:\n",
        "        print('-',column_name,\"in dataset:\")\n",
        "        [print(value) for value in column_values]\n",
        "\n",
        "    if check_NaN:\n",
        "        print(\"-NaN  present in the column?\",dataset[column_name].isnull().any())\n",
        "    \n",
        "    if group_count:\n",
        "        values_count = dataset[column_name].value_counts()   \n",
        "        result.update(values_count=values_count)\n",
        "        \n",
        "    #histogram of values\n",
        "    if plot_hist:\n",
        "        fig = hist(dataset[column_name], column_name) \n",
        "        fig.show()        \n",
        "    \n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6J1JjYj9jkH4"
      },
      "source": [
        "result = get_basic_stats(\"race\",plot_hist=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "QpVT42IAjkH5"
      },
      "source": [
        "result = get_basic_stats(\"age\", plot_hist=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsEIRtpnjkH5"
      },
      "source": [
        "sex_race_df = pd.crosstab(dataset.race, dataset.sex)\n",
        "sex_race_df = sex_race_df.stack().reset_index().rename(columns={0:\"value\"})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2znoGtOjkH5"
      },
      "source": [
        "race = np.sort(dataset.race.unique())\n",
        "males = sex_race_df.value[sex_race_df.sex == 'Male']\n",
        "females = sex_race_df.value[sex_race_df.sex == 'Female']\n",
        "\n",
        "fig = go.Figure(data=[\n",
        "    go.Bar(name='Male', x=race, y= males),\n",
        "    go.Bar(name='Female', x=race, y=females)\n",
        "])\n",
        "# Change the bar mode\n",
        "fig.update_layout(barmode='stack')\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "ccH85bhdjkH6"
      },
      "source": [
        "decile_score_per_race = pd.crosstab(dataset.decile_score, dataset.race)\n",
        "decile_score_per_race = decile_score_per_race.stack().reset_index().rename(columns = {0:'values'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "MWWO5wX0jkH6"
      },
      "source": [
        "dec_score = dataset.decile_score.unique()\n",
        "afrac_amer = decile_score_per_race.values[decile_score_per_race.race == \"African-American\"][:,2]\n",
        "hispanic = decile_score_per_race.values[decile_score_per_race.race == \"Hispanic\"][:,2]\n",
        "asian = decile_score_per_race.values[decile_score_per_race.race == \"Asian\"][:,2]\n",
        "causcasian = decile_score_per_race.values[decile_score_per_race.race == \"Caucasian\"][:,2]\n",
        "native = decile_score_per_race.values[decile_score_per_race.race == \"Native American\"][:,2]\n",
        "other = decile_score_per_race.values[decile_score_per_race.race == \"Other\"][:,2]\n",
        "\n",
        "fig = go.Figure(data = [\n",
        "    go.Bar(name = 'African-American', x = dec_score, y = afrac_amer),\n",
        "    go.Bar(name = 'Hispanic', x = dec_score, y = hispanic),\n",
        "    go.Bar(name = 'Asian', x = dec_score, y = asian),\n",
        "    go.Bar(name = 'Causcasian', x = dec_score, y = causcasian),\n",
        "    go.Bar(name = 'Native-American', x = dec_score, y = native),\n",
        "    go.Bar(name = 'Other', x = dec_score, y = other)\n",
        "])\n",
        "fig.update_layout(barmode='stack')\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRyYvb_XjkH7"
      },
      "source": [
        "The *low*, *medium* and *high* values below are categories for risk based on the **decile_score** computed. This will be discussed in more detail in the upcoming sections."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "SzgksdT3jkH7"
      },
      "source": [
        "score_text_per_race = pd.crosstab(dataset.score_text, dataset.race)\n",
        "score_text_per_race = score_text_per_race.stack().reset_index().rename(columns = {0:'values'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIG87rosjkH7"
      },
      "source": [
        "dec_score = dataset.score_text.unique()\n",
        "afrac_amer = score_text_per_race.values[score_text_per_race.race == \"African-American\"][:,2]\n",
        "hispanic = score_text_per_race.values[score_text_per_race.race == \"Hispanic\"][:,2]\n",
        "asian = score_text_per_race.values[score_text_per_race.race == \"Asian\"][:,2]\n",
        "causcasian = score_text_per_race.values[score_text_per_race.race == \"Caucasian\"][:,2]\n",
        "native = score_text_per_race.values[score_text_per_race.race == \"Native American\"][:,2]\n",
        "other = score_text_per_race.values[score_text_per_race.race == \"Other\"][:,2]\n",
        "\n",
        "fig = go.Figure(data = [\n",
        "    go.Bar(name = 'African-American', x = dec_score, y = afrac_amer),\n",
        "    go.Bar(name = 'Hispanic', x = dec_score, y = hispanic),\n",
        "    go.Bar(name = 'Asian', x = dec_score, y = asian),\n",
        "    go.Bar(name = 'Causcasian', x = dec_score, y = causcasian),\n",
        "    go.Bar(name = 'Native-American', x = dec_score, y = native),\n",
        "    go.Bar(name = 'Other', x = dec_score, y = other)\n",
        "])\n",
        "fig.update_layout(barmode='stack')\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8KEGlKLjkH7"
      },
      "source": [
        "### Decile score vs Race\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUnpp9LgjkH7"
      },
      "source": [
        "def scores_stats(category, value):\n",
        "    '''\n",
        "    @param category: vategory to perform stats on\n",
        "    @param value: entry in the specefied category\n",
        "    '''\n",
        "    scores = dataset[dataset[category] == value]\n",
        "    scores = scores[['decile_score','score_text']]\n",
        "    histogram,_ = np.histogram(scores['decile_score'])\n",
        "    fig = px.histogram(scores['decile_score'], 'decile_score',\n",
        "                       title = 'Decile score of '+ value + 's',\n",
        "                       opacity = 0.8,\n",
        "                       color_discrete_sequence = ['indianred'])\n",
        "    fig.add_shape(dict(\n",
        "        type=\"line\",\n",
        "        x0 = scores['decile_score'].mean(),\n",
        "        y0 = 0,\n",
        "        x1 = scores['decile_score'].mean(),\n",
        "        y1 = max(histogram) + 20,\n",
        "        line=dict(\n",
        "            color = \"LightSeaGreen\",\n",
        "            width = 4,\n",
        "            dash = \"dashdot\")\n",
        "    ))\n",
        "    fig.show()\n",
        "    print('mean decile score: %.2f'%scores['decile_score'].mean())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTqEh1YtjkH7"
      },
      "source": [
        "scores_stats('race','African-American')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5KyTigcjkH8"
      },
      "source": [
        "scores_stats('race','Caucasian')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lMZG3zSjkH8"
      },
      "source": [
        "### Decile score vs Gender\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "_g5rz9v-jkH8"
      },
      "source": [
        "scores_stats('sex','Male')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4o_1YFSMjkH8"
      },
      "source": [
        "scores_stats('sex','Female')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tcktxvv5sqGr"
      },
      "source": [
        "###Ground truth data - How much recidivism is there in our dataset?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5XPlttgjkH9"
      },
      "source": [
        "Now, we are going to look at COMPAS's distribution with regards to the number of people who were re-arrested in two years (i.e. what COMPAS aims to predict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyilqZdwjkH9"
      },
      "source": [
        "#### Number of people who were re-arrested\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9eyF05yjkH9"
      },
      "source": [
        "#people whom got re-arrested in two years span\n",
        "print('Number of people in the dataset whom got re-arrested in the span of two years: ',\n",
        "      dataset['two_year_recid'].sum())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBzw2KnujkH9"
      },
      "source": [
        "#### Number of people who were rearrested, split by race and gender"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OwdQs2tjkH_"
      },
      "source": [
        "two_recide_sex_race_df = pd.crosstab(dataset.race[dataset.two_year_recid == 1], dataset.sex[dataset.two_year_recid == 1])\n",
        "two_recide_sex_race_df = two_recide_sex_race_df.stack().reset_index().rename(columns={0:\"value\"})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "pYKrXmq5jkH_"
      },
      "source": [
        "race = dataset.race.unique()\n",
        "race = np.sort(race)\n",
        "males = two_recide_sex_race_df.value[two_recide_sex_race_df.sex == 'Male']\n",
        "females = two_recide_sex_race_df.value[two_recide_sex_race_df.sex == 'Female']\n",
        "\n",
        "fig = go.Figure(data=[\n",
        "    go.Bar(name='Male', x=race, y= males),\n",
        "    go.Bar(name='Female', x=race, y=females)\n",
        "], layout_title_text = \"Male/Female Recidivism by Race\")\n",
        "# Change the bar mode\n",
        "fig.update_layout(barmode='stack')\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCw5cptNjkH_"
      },
      "source": [
        "# Plot confusion matrix without passing the classifier\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from itertools import cycle, product\n",
        "def plot_confusion_matrix(cm, classes, title='Confusion Matrix', cmap=plt.cm.Blues):\n",
        "    print(cm)\n",
        "    \n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "    \n",
        "    fmt = 'd'\n",
        "    thresh = cm.max() / 2\n",
        "    for i, j in product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                horizontalalignment=\"center\",\n",
        "                color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    \n",
        "    \n",
        "def plot_conf_matrix(y_test, y_pred, class_names):\n",
        "    print(\"plotting the Confusion Matrix\")\n",
        "    cnf_matrix = confusion_matrix(y_test, y_pred)\n",
        "    np.set_printoptions(precision=2)\n",
        "    print('CCR = {}'.format(np.trace(cnf_matrix) / len(y_test)))\n",
        "    print('Precision = {}'.format(precision_macro_average(cnf_matrix)))\n",
        "    print('Recall = {}'.format(recall_macro_average(cnf_matrix)))\n",
        "    plt.figure()\n",
        "    plot_confusion_matrix(cnf_matrix, classes=class_names, title='Confusion Matrix')\n",
        "    plt.show()\n",
        "    \n",
        "def recall_macro_average(confusion_matrix):\n",
        "    rows, columns = confusion_matrix.shape\n",
        "    sum_of_recalls = 0\n",
        "    for label in range(columns):\n",
        "        sum_of_recalls += recall(label, confusion_matrix)\n",
        "    return sum_of_recalls / columns\n",
        "\n",
        "def precision_macro_average(confusion_matrix):\n",
        "    rows, columns = confusion_matrix.shape\n",
        "    sum_of_precisions = 0\n",
        "    for label in range(rows):\n",
        "        sum_of_precisions += precision(label, confusion_matrix)\n",
        "    return sum_of_precisions / rows\n",
        "\n",
        "def precision(label, confusion_matrix):\n",
        "    col = confusion_matrix[:, label]\n",
        "    return confusion_matrix[label, label] / col.sum()\n",
        "    \n",
        "def recall(label, confusion_matrix):\n",
        "    row = confusion_matrix[label, :]\n",
        "    return confusion_matrix[label, label] / row.sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ldPTjIuExHC"
      },
      "source": [
        "### **Question 1: Takeaways from the distributions**\n",
        "Now that you have looked at the way this data is distributed, what do you notice? What imbalances (if any) do you see and how might they affect the results of a model that uses this dataset? Answer these questions in the cell below:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTm9xL45sWcL"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKIyya28jkIB"
      },
      "source": [
        "### Error Analysis of COMPAS Scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8bCokgWmv1z"
      },
      "source": [
        "Below, we look at the error rates, specifically the false positive and false negative rates, for Caucasians and African Americans. In order to calculate these error rates, we need some sort of binary classification from these decile scores. To get that, we can use a *threshold* such that everything above that threshold is classified as a prediction that the person will commit a reoffense in two years. For instance, if we picked 7, we can say any decile score of 7 or higher will be a prediction that the person will reoffend. Here, we plot the false positive and false negative rates for every possible threshold based on the decile scores.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20UECkmOiRtu"
      },
      "source": [
        "#get false negative and false positive rates \n",
        "compas = pd.read_csv(COMPAS_INPUT_FILE).query('days_b_screening_arrest <= 30 & days_b_screening_arrest >= -30')\n",
        "\n",
        "compas_violent = compas[compas['race'].isin(['African-American', 'Caucasian'])].groupby(['race', 'v_decile_score'], as_index=False)\n",
        "compas = compas[compas['race'].isin(['African-American', 'Caucasian'])].groupby(['race', 'decile_score'], as_index=False)\n",
        "\n",
        "compas_stats = compas['two_year_recid'].agg({'recid_count': recid_count_fn, 'non_recid_count': non_recid_count_fn, 'total_count': total_count_fn})\n",
        "compas_violent_stats = compas_violent['two_year_recid'].agg({'recid_count': recid_count_fn, 'non_recid_count': non_recid_count_fn, 'total_count': total_count_fn})\n",
        "\n",
        "#add columns to compas stats so we can find tpr, fpr, tnr, fnr\n",
        "for race in ['African-American', 'Caucasian']:\n",
        "    data = compas_stats['race'] == race\n",
        "    compas_stats.loc[data, 'TP'] = TP(compas_stats[data]) \n",
        "    compas_stats.loc[data, 'TN'] = TN(compas_stats[data]) \n",
        "    compas_stats.loc[data, 'FP'] = FP(compas_stats[data]) \n",
        "    compas_stats.loc[data, 'FN'] = FN(compas_stats[data])\n",
        "    compas_stats.fillna(0 , inplace=True)\n",
        "\n",
        "fpr = compas_stats['FP'] / (compas_stats['FP'] + compas_stats['TN']).tolist()\n",
        "fnr = compas_stats['FN'] / (compas_stats['FN'] + compas_stats['TP']).tolist()\n",
        "fpr_length = int(len(fpr)/2); fnr_length = int(len(fnr)/2)\n",
        "fpr_african_americans = fpr[:fpr_length]; fpr_caucasians = fpr[fpr_length:]\n",
        "fnr_african_americans = fnr[:fnr_length]; fnr_caucasians = fnr[fnr_length:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fF-dfKF6gOEK"
      },
      "source": [
        "#visualize false positive rates\n",
        "\n",
        "decileScores = np.arange(1,11)\n",
        "\n",
        "fig = go.Figure(data=[\n",
        "    go.Bar(name='Caucasian', x=decileScores, y=fpr_caucasians[::-1]),\n",
        "    go.Bar(name='African American', x=decileScores, y=fpr_african_americans[::-1]),\n",
        "], layout_title_text = \"False Positive Rate for Diff. Thresholds: African Americans vs. Caucasians\")\n",
        "# Change the bar mode\n",
        "fig.update_layout(barmode='stack')\n",
        "fig.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mb71tMGJmU3V"
      },
      "source": [
        "#visualize false negative rates\n",
        "\n",
        "decileScores = np.arange(1,11)\n",
        "\n",
        "fig = go.Figure(data=[\n",
        "    go.Bar(name='Caucasian', x=decileScores, y=fnr_caucasians),\n",
        "    go.Bar(name='African American', x=decileScores, y=fnr_african_americans),\n",
        "], layout_title_text = \"False Negative Rate for Diff. Thresholds: African Americans vs. Caucasians\")\n",
        "# Change the bar mode\n",
        "fig.update_layout(barmode='stack')\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CktV3nVGqe-b"
      },
      "source": [
        "###**Question 2: What do you notice about these error rates? Do you see any biases? What could potentially happen as a result of these biases?**\n",
        "Answer this question in the cell below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFA7_9t6oz_G"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hide_input": true,
        "id": "orRbC45ojkIC"
      },
      "source": [
        "## Part 2.2 - Standard classifiers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_u4IUHNjkID"
      },
      "source": [
        "In this section, we are going to train a simple SVM classifier on the \"two_years_recid\" ground truth data and plot confusion matrices in order to investigate the bias in the classifiers towards different attributes. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "U8jMAte8jkIA"
      },
      "source": [
        "#mapping of low medium high to decile score\n",
        "print('decile score ranges in score text')\n",
        "scores = dataset[['decile_score','score_text']]\n",
        "low_range = scores[scores['score_text'] == 'Low']['decile_score']\n",
        "print('low:',low_range.min(),low_range.max())\n",
        "medium_range = scores[scores['score_text'] == 'Medium']['decile_score']\n",
        "print('medium:',medium_range.min(),medium_range.max())\n",
        "High_range = scores[scores['score_text'] == 'High']['decile_score']\n",
        "print('high',High_range.min(),High_range.max())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xhoeZufjkIA"
      },
      "source": [
        "To be able to compare compas scores in range [1,10], to the ground truth binary 0/1, the COMPAS scores are grouped into two groups to transfer them into binary labels\n",
        "- *recidivate* means *high* = label *1*\n",
        "- *will not recidivate* means *low/medium* = label *0*\n",
        "\n",
        "Thus, here, our threshold is .8"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emlLK4-djkIA"
      },
      "source": [
        "# split scores to new classes, high to 1 and medium/ low to zero\n",
        "new_scores = dataset.copy()\n",
        "new_scores['decile_score'].replace(to_replace=range(1,8),value=0,inplace=True)\n",
        "new_scores['decile_score'].replace(to_replace=range(8,11),value=1,inplace=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VNQ0CwTjkID"
      },
      "source": [
        "- Preparing for the next part, we start using IBM aif360 package, a collection of fairness algorithms and performance metrics  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vyz6nxTjkID"
      },
      "source": [
        "- We use the following set of features out of the 137 questions collected by northepoint, we notice that we are including the charge degree(Felony/murder), the priors count(number of crimes commited before), and age category."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIUB1Pt_jkID"
      },
      "source": [
        "#note: to run this line, you may need to put the compas-scores-two-years.csv file in the following directory:\n",
        "#/usr/local/lib/python3.6/dist-packages/aif360/data/raw/compas\n",
        "\n",
        "dataset_conditioned = load_preproc_data_compas()\n",
        "print(\"set of features to be used out of the dataset\")\n",
        "pprint(dataset_conditioned.feature_names)    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2lcWAVxjkID"
      },
      "source": [
        "- As a normalization step, all of the data values are turned into binary values, in order to equalize the effect of features with respect to the classifier.\n",
        "\n",
        "- The race has more than one value, but since we want to look at the bias between African American and Caucasians, we drop the other races from the dataset. This does not introduce a problem, since the size of the dataset drops from 6127 entries to 5278 entries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-iEfCldjkID"
      },
      "source": [
        "dataset_as_df,_ = dataset_conditioned.convert_to_dataframe()\n",
        "print(\"Example samples from the dataset\")\n",
        "dataset_as_df.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kV6r9UdqjkIE"
      },
      "source": [
        "### Train-validate-test split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIHuG6PyjkIE"
      },
      "source": [
        "- We divide our dataset into 3 folds to train/test different classifers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVEnZK_HjkIE"
      },
      "source": [
        "dataset_conditioned_train, dataset_conditioned_validate_test = dataset_conditioned.split([0.7], shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktN8IwHZjkIE"
      },
      "source": [
        "print('size of traning and testing samples')\n",
        "dataset_conditioned_train.convert_to_dataframe()[0].shape,dataset_conditioned_validate_test.convert_to_dataframe()[0].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCwi-cudzITz"
      },
      "source": [
        "test_df = dataset_conditioned_validate_test.convert_to_dataframe()[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyBnF_GdjkIE"
      },
      "source": [
        "### Splitting based on gender "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPiLmCq7jkIE"
      },
      "source": [
        "# get male samples from the dataset\n",
        "male_samples = test_df[test_df['sex'] == 0.0]\n",
        "male_samples_label = male_samples.filter(['two_year_recid'])\n",
        "male_samples.drop(columns=['two_year_recid'],inplace=True)\n",
        "\n",
        "# get female samples from the dataset\n",
        "female_samples = test_df[test_df['sex'] == 1.0]\n",
        "female_samples_label = female_samples.filter(['two_year_recid'])\n",
        "female_samples.drop(columns=['two_year_recid'],inplace=True)\n",
        "print('Number of female samples in test set: {0}, number of male samples in test set: {1}'.format(female_samples.shape,\n",
        "                                                                                      male_samples.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHYYcwzSjkIE"
      },
      "source": [
        "### Splitting based on race - African American and Caucasian"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yP-1vKqWjkIF"
      },
      "source": [
        "\n",
        "#get caucasians samples from the dataset\n",
        "caucasian_samples = test_df[test_df['race'] == 1.0]\n",
        "caucasian_samples_label = caucasian_samples.filter(['two_year_recid'])\n",
        "caucasian_samples.drop(columns=['two_year_recid'],inplace=True)\n",
        "\n",
        "#get african american samples from the dataset\n",
        "african_american_samples = test_df[test_df['race'] == 0.0]\n",
        "african_american_samples_label = african_american_samples.filter(['two_year_recid'])\n",
        "african_american_samples.drop(columns=['two_year_recid'],inplace=True)\n",
        "print('Number of African American samples in test set: {0}, number of Caucasian samples in test set: {1}'.format(african_american_samples.shape,\n",
        "                                                                                      caucasian_samples.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4woIPw9njkIF"
      },
      "source": [
        "### Accuracy of COMPAS Scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3r0skLaZjkIF"
      },
      "source": [
        "Before delving into the performance of our SVM classifier, let's look at the confusion matrices with respect to race and gender for our sample and chosen threshold in order to inspect the true positive rate and false positive rates."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QqtCzqrjkIB"
      },
      "source": [
        "overal_accuracy = performance.accuracy_score(new_scores['two_year_recid'],\n",
        "                                             new_scores['decile_score'],normalize=True)\n",
        "\n",
        "print('COMPAS overall performance against the true labels: {0:3f}%'.format(overal_accuracy *100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6v_TU8CjkIF"
      },
      "source": [
        "#### Confusion Matrix - African American\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTIQ5Q46jkIF"
      },
      "source": [
        "new_scores_african_american = new_scores[new_scores['race'] == 'African-American']\n",
        "performance.accuracy_score(new_scores_african_american['two_year_recid'],new_scores_african_american['decile_score'],normalize=True)\n",
        "\n",
        "# performance.confusion_matrix(new_scores_african_american['two_year_recid'],new_scores_african_american['decile_score'])\n",
        "plot_conf_matrix(new_scores_african_american['two_year_recid'], new_scores_african_american['decile_score'], class_names=['Not Recedivate', 'Recedivate'] )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSjWa9wzi8AG"
      },
      "source": [
        "####Confusion Matrix - Caucasian"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fD3KjYYi5-U"
      },
      "source": [
        "new_scores_Caucasian = new_scores[new_scores['race'] == 'Caucasian']\n",
        "# performance.accuracy_score(new_scores_Caucasian['two_year_recid'],new_scores_Caucasian['decile_score'],normalize=True)\n",
        "plot_conf_matrix(new_scores_Caucasian.two_year_recid, new_scores_Caucasian.decile_score, class_names=['Not Recedivate', 'Recedivate'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLjABvThjkIF"
      },
      "source": [
        "#### Confusion Matrix - Female"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nK_6aGkijkIF"
      },
      "source": [
        "new_scores_female = new_scores[new_scores['sex'] == 'Female']\n",
        "plot_conf_matrix(new_scores_female.two_year_recid, new_scores_female.decile_score, class_names=['Not Recedivate', 'Recedivate'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diiv5IQzppNF"
      },
      "source": [
        "####Confusion Matrix - Male"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMZZI4ACp0sX"
      },
      "source": [
        "new_scores_male = new_scores[new_scores['sex'] == 'Male']\n",
        "plot_conf_matrix(new_scores_male.two_year_recid, new_scores_male.decile_score, class_names=['Not Recedivate', 'Recedivate'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-T0AKG-qCrQ"
      },
      "source": [
        "Next, we examine the results of running three simple classifiers on the COMPAS data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgIqgQvQjkIG"
      },
      "source": [
        "### SVM\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFG5NarwjkIG"
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "svm_classifier = SVC(gamma=0.001, C=100.)\n",
        "svm_classifier.fit(dataset_conditioned_train.features, dataset_conditioned_train.labels.ravel())\n",
        "\n",
        "pred = svm_classifier.predict(dataset_conditioned_validate_test.features)\n",
        "\n",
        "print('Accuracy of the SVM on testing set {0:.2f}% '\n",
        "      .format(performance.accuracy_score(dataset_conditioned_validate_test.labels, pred, normalize=True)*100))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3YuSu4ujkIG"
      },
      "source": [
        "#### Confusion Matrix for African Americans"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbPuVcIVjkIG"
      },
      "source": [
        "african_american_prediction = svm_classifier.predict(african_american_samples)\n",
        "plot_conf_matrix(african_american_samples_label.to_numpy().ravel(), african_american_prediction, class_names=['Not Recedivate', 'Recedivate'] )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcy34QV4jkIG"
      },
      "source": [
        "#### Confusion Matrix for Caucasians"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIYPw9WKjkIG"
      },
      "source": [
        "caucasian_prediction = svm_classifier.predict(caucasian_samples)\n",
        "plot_conf_matrix(caucasian_samples_label.to_numpy().ravel(), caucasian_prediction, class_names=['Not Recedivate', 'Recedivate'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nABuudxc0Gpf"
      },
      "source": [
        "####Confusion Matrix for Females"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_m68drL0Ppg"
      },
      "source": [
        "female_prediction = svm_classifier.predict(female_samples)\n",
        "plot_conf_matrix(female_samples_label.to_numpy().ravel(), female_prediction, class_names=['Not Recedivate', 'Recedivate'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TH4CbSF20K7j"
      },
      "source": [
        "####Confusion Matrix for Males"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MuaM3Mve0WPG"
      },
      "source": [
        "male_prediction = svm_classifier.predict(male_samples)\n",
        "plot_conf_matrix(male_samples_label.to_numpy().ravel(), male_prediction, class_names=['Not Recedivate', 'Recedivate'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFc1QZGtjkIK"
      },
      "source": [
        "### **Question 3: What biases did the classifier show?**\n",
        "After training and testing this SVM classifier, what did you notice about the results above? How did the confusion matrices from the output of the classifiers compare to those from the COMPAS data? What kinds of biases did both show? Answer these questions in the cell below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtQA1g3kjkIK"
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nryVxMw_jkIL"
      },
      "source": [
        "## Building fairer classifiers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLQPqbB8jkIL"
      },
      "source": [
        "### **Question 4: How can these classifiers be improved?**\n",
        "What modifications can be made on the above classifiers that would result in fairer outcomes? Write some potential modifications in the cell below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNW-kYdYKCI8"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P15azilhjkIQ"
      },
      "source": [
        "###**Question 5 - Implement a Fairer classifier**\n",
        "Using your suggested modifications above, modify one of the simple classifiers in order to produce a fairer model. Implement this model and plot the resulting ROC curves below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocjxkuy-Kxd1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtdSHHFrXhn3"
      },
      "source": [
        "###**Extra Credit: Implement a deep learning based model**\n",
        "For extra credit, implement a fair deep learning based model in the cell below and measure its performance using confusion matrices (you can add extra cells if needed). How did its results compare to those of the fair SVM classifier? Write a one-sentence analysis of the results\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wSCclGVX9u9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}